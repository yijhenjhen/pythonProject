{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelTrain.ipynb",
      "provenance": [],
      "mount_file_id": "1UObtG2FoCYldnOqWnbrkkaFa77_r61me",
      "authorship_tag": "ABX9TyOUhXlQPxKhVfKipjce+6q7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yijhenjhen/pythonProject/blob/master/modelTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV4LtqQ0UKrz",
        "outputId": "7ef64ee6-9af9-4b1d-8eec-0cd1ed654df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 28.2 MB of archives.\n",
            "After this operation, 104 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u312-b07-0ubuntu1~18.04 [28.2 MB]\n",
            "Fetched 28.2 MB in 3s (10.7 MB/s)\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 52.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805911 sha256=8b236d7f8c00a83f1bc50769708d8528857a7c93413c2f047a106d406aece3d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install openjdk-8-jre-headless\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.feature import VectorAssembler,VectorIndexer, OneHotEncoder, StringIndexer\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF\n",
        "from pyspark.ml.linalg import Vector\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import lower, length, size\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "usyX-ZwoUWK8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "rveSjAGFUY_P",
        "outputId": "057063c4-b26c-4aee-f062-fd169ac56cf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://67df615b2879:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6faef09050>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "cwpsS1XNUZnA",
        "outputId": "c78eb379-2bf4-428e-f0b6-4746cd912818"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://67df615b2879:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=pyspark-shell>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for multi-label classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "8uakkic5Uabz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv(\"/content/drive/MyDrive/Colab Notebooks/oriDataTag.csv\",\n",
        "                      inferSchema=True,\n",
        "                      header=True,\n",
        "                      encoding='utf-8')\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset(data):\n",
        "\t\n",
        "    # Preprocessing and feature engineering\n",
        "    feature_prep_ori = data.select('sex', 'job', 'age', 'market', 'risk', 'joinTime', 'content', 'tag').orderBy('tag')\n",
        "    feature_prep = feature_prep_ori.dropna(how='any', thresh=None, subset=None)\n",
        "    feature_prep = StringIndexer(inputCol=\"sex\", outputCol=\"SexIndex\").fit(feature_prep).transform(feature_prep)\n",
        "    feature_prep = StringIndexer(inputCol=\"job\",outputCol=\"jobIndex\").fit(feature_prep).transform(feature_prep)\n",
        "    feature_prep = StringIndexer(inputCol=\"joinTime\",outputCol=\"joinTimeIndex\").fit(feature_prep).transform(feature_prep)\n",
        "    feature_prep = StringIndexer(inputCol=\"risk\",outputCol=\"riskIndex\").fit(feature_prep).transform(feature_prep)\n",
        "\n",
        "    data = feature_prep.toPandas()\n",
        "    data.to_csv('/content/drive/MyDrive/Colab Notebooks/dataTrain.csv', sep=',', encoding='utf_8_sig', index=True)\n",
        "    dataTrain=data.sample(frac=0.9)\n",
        "    dataTest=data[~data.index.isin(dataTrain.index)]\n",
        "    dataframe = dataTrain[['SexIndex', 'jobIndex', 'age', 'riskIndex', 'joinTimeIndex', 'tag']]\n",
        "\n",
        "\n",
        "    # 建立label的numpy array\n",
        "    featuresArray = []\n",
        "    labelArray = []\n",
        "    df = dataframe['tag']\n",
        "    k=0\n",
        "\n",
        "    for i in df:\n",
        "        index = (df.index)[k]\n",
        "        featuresVec = [0] * 5\n",
        "        featuresVec[0] = dataframe.loc[index, 'SexIndex']\n",
        "        featuresVec[1] = dataframe.loc[index, 'jobIndex']\n",
        "        featuresVec[2] = dataframe.loc[index, 'age']\n",
        "        featuresVec[3] = dataframe.loc[index, 'riskIndex']\n",
        "        featuresVec[4] = dataframe.loc[index, 'joinTimeIndex']\n",
        "        featuresArray.append(np.array(featuresVec))\n",
        "\n",
        "        tagVec = [0] * 13\n",
        "        for j in i.split(','):\n",
        "\n",
        "            try:\n",
        "                j = int(j)\n",
        "                if j < 14:\n",
        "                    tagVec[j-1] = 1\n",
        "                      #data.loc[index, 'tag'] = str(tagVec)\n",
        "                else:\n",
        "                    tagVec[j-1] = 1\n",
        "                      #data.loc[index, 'tag'] = str(tagVec)\n",
        "            \n",
        "            # print(np.array(tagVec))\n",
        "            except:\n",
        "                pass\n",
        "        labelArray.append(np.array(tagVec))\n",
        "        k+=1\n",
        "\n",
        "    featuresArray = np.array(featuresArray)\n",
        "    labelArray = np.array(labelArray)\n",
        "\n",
        "\n",
        "    # summarize dataset shape\n",
        "    print(featuresArray.shape, labelArray.shape)\n",
        "    # summarize first few examples\n",
        "    for i in range(10):\n",
        "        print(featuresArray[i], labelArray[i])\n",
        "\n",
        "    return featuresArray, labelArray, dataTest"
      ],
      "metadata": {
        "id": "LrgvkKFJUiXG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the model\n",
        "def get_model(n_inputs, n_outputs):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=n_inputs, activation='relu'))# kernel_initializer='he_uniform', \n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))# kernel_initializer='he_uniform',\n",
        "    # model.add(Dropout(0.1))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "G5FJTN3GXMa0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a model using repeated k-fold cross-validation\n",
        "from sklearn.utils import class_weight\n",
        "def evaluate_model(X, y):\n",
        "\n",
        "\tresults = list()\n",
        "\t\n",
        "\ttime=0\n",
        "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "\tprint(X.shape[1], y.shape[1])\n",
        "\n",
        "\t# define evaluation procedure\n",
        "\t# cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
        "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\t\n",
        "\n",
        "\t# for i in range(epoch):\n",
        "\t# \tmodel = get_model(n_inputs, n_outputs)\n",
        "\t# \tmodel.fit(X_train, y_train, verbose=2, epochs=100)\n",
        "\t# \tyhat = model.predict(X_test)\n",
        "\t# \tyhat = yhat.round()\n",
        "\t# \tacc = accuracy_score(y_test, yhat)\n",
        "\t# \tprint('>%.3f' % acc)\n",
        "\t# \tresults.append(acc)\n",
        "\t\n",
        "\t# enumerate folds\n",
        "\t# y_pre = np.zeros((179, 13))\n",
        "\tmodel = get_model(n_inputs, n_outputs)\n",
        "\n",
        "\tclass_weight = {0:1.87, 1:4.51, 2:0.4, 3:1.8, 4:1.81, 5:0.5, 6:5,\n",
        "\t         7:2.99, 8:1.81, 9:8, 10:8, 11:4.2, 12:2.28}\n",
        "\tmodel.fit(X_train, y_train, verbose=0, epochs=1000)\n",
        " \n",
        "\tyhat = model.predict(X_test)\n",
        "\t# for train_ix, test_ix in cv.split(X):\n",
        "\t# \t# prepare data\n",
        "\t# \tX_train, X_test = X[train_ix], X[test_ix]\n",
        "\t# \ty_train, y_test = y[train_ix], y[test_ix]\n",
        "\t# \t# define model\n",
        "\t# \tmodel = get_model(n_inputs, n_outputs)\n",
        "\t# \t# fit model\n",
        "\t# \tmodel.fit(X_train, y_train, verbose=0, epochs=10)\n",
        "\t# \t# make a prediction on the test set\n",
        "\t# \tyhat = model.predict(X_test)\n",
        "\t\t\n",
        "\t\t# round probabilities to class labels\n",
        "\t\t# for yn in range(len(yhat)):\n",
        "\t\t# \tindex=0\n",
        "\t\t# \t# for iyn in yhat[yn]:\n",
        "\t\t\t\t\n",
        "\t\t# \t\t# print(iyn)\n",
        "\t\t# \tprint(yhat[yn].round(4))\n",
        "\t\t\t\t# index += 1\n",
        "\t\t# print('#####################################################################################')\n",
        "\t\t# yhat = yhat.round()\n",
        "\t\t# # print(yhat.round(4))\n",
        "\t\t# # calculate accuracy\n",
        "\t\t# acc = accuracy_score(y_test, yhat)\n",
        "\t\t# # store result\n",
        "\t\t# print('>%.3f' % acc)\n",
        "\t\t# results.append(acc)\n",
        "\t\t# #y_pre.append(yhat)\n",
        "\t\t# y_pre+=yhat\n",
        "\t\t# time+=1\n",
        "\t#y_pre=y_pre/time\n",
        "\t# for i in range(len(yhat)):\n",
        "\t# \tprint(yhat[i])\n",
        "\n",
        "\t\n",
        "\t\t# print(yhat[i].round(4))\n",
        "\tmodel.save(\"/content/drive/MyDrive/Colab Notebooks/modelTag_V3.h5\")\n",
        "\treturn results"
      ],
      "metadata": {
        "id": "kz6_IfqpXghd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "X, y, dataset = get_dataset(data)\n",
        "# evaluate model\n",
        "results = evaluate_model(X, y)\n",
        "# summarize performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(results), std(results)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1IcK7kwZAhF",
        "outputId": "f7ad654b-5a27-4c42-d146-bd70a7bdff68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1608, 5) (1608, 13)\n",
            "[ 0.  7. 44.  0.  2.] [0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
            "[ 0.  9. 33.  0.  0.] [0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "[ 0. 20. 38.  0.  0.] [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[ 0. 16. 24.  0.  0.] [1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "[ 0.  6. 41.  2.  2.] [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "[ 0. 11. 40.  1.  2.] [0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
            "[ 0.  0. 37.  0.  3.] [0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[ 0.  6. 48.  0.  4.] [0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "[ 0.  6. 47.  0.  1.] [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "[ 0.  0. 42.  0.  0.] [0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
            "5 13\n",
            "Accuracy: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
            "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import random\n",
        "pd.options.display.max_columns = None\n",
        "test = list()\n",
        "\n",
        "\n",
        "X, y, dataset = get_dataset(data)\n",
        "dataset.reset_index(inplace=True, drop=False)\n",
        "for t in range(178):\n",
        "    x = random.randrange(0, 178)\n",
        "    test.append(x)\n",
        "\n",
        "\n",
        "dataframe = dataset[['SexIndex', 'jobIndex', 'age', 'riskIndex', 'joinTimeIndex', 'tag']]\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/modelTag_1.h5',compile=False)\n",
        "acc=0\n",
        "for i in test:\n",
        "    # print(i)\n",
        "\n",
        "    # index = (dataset['index'].index)[0]\n",
        "    featuresVec = [0] * 5\n",
        "    featuresVec[0] = dataset.loc[i, 'SexIndex']\n",
        "    featuresVec[1] = dataset.loc[i, 'jobIndex']\n",
        "    featuresVec[2] = dataset.loc[i, 'age']\n",
        "    featuresVec[3] = dataset.loc[i, 'riskIndex']\n",
        "    featuresVec[4] = dataset.loc[i, 'joinTimeIndex']\n",
        "\n",
        "    input = np.array(featuresVec).reshape((1,5))\n",
        "    y_predicted = model.predict(input)\n",
        "    # print(y_predicted.round(4))\n",
        "    # print(dataset[i:i+1])\n",
        "\n",
        "    resh = y_predicted[0]\n",
        "    top_k = 3\n",
        "    top_k_idx = resh.argsort()[::-1][0:top_k]\n",
        "    top_k_idx += 1\n",
        "    # print(\"困擾類型:\", top_k_idx)\n",
        "    right=0\n",
        "\n",
        "    nums=dataset.loc[i, 'tag'].split(',')\n",
        "    for k in nums:\n",
        "      if int(float(k)) in top_k_idx:\n",
        "        right+=1\n",
        "      else:\n",
        "        pass\n",
        "    correct = right/len(nums)\n",
        "    print(correct)\n",
        "    acc+=correct\n",
        "    # print('####################################################################')\n",
        "print(\"ACC:\",acc/len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfH2viBGZD2l",
        "outputId": "a33a5930-ee2d-43de-d2e7-3d4c2d5916d7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1608, 5) (1608, 13)\n",
            "[ 0.  5. 42.  0.  1.] [0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "[ 0.  2. 36.  3.  0.] [0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
            "[ 0.  7. 58.  0.  0.] [0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
            "[ 0.  1. 59.  2.  2.] [0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "[ 0.  8. 42.  2.  0.] [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "[ 0.  1. 40.  0.  4.] [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "[ 0.  9. 38.  7.  1.] [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            "[ 0.  0. 36.  1.  2.] [0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "[ 0.  0. 41.  0.  4.] [0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "[ 0.  1. 20.  0.  1.] [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "0.0\n",
            "0.5\n",
            "1.0\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "0.6666666666666666\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.5\n",
            "0.5\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.5\n",
            "0.3333333333333333\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.3333333333333333\n",
            "0.0\n",
            "0.6666666666666666\n",
            "0.6666666666666666\n",
            "0.5\n",
            "0.0\n",
            "0.6666666666666666\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.6666666666666666\n",
            "0.6666666666666666\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.5\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.5\n",
            "0.6666666666666666\n",
            "1.0\n",
            "0.5\n",
            "1.0\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "0.6666666666666666\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.5\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.6666666666666666\n",
            "0.0\n",
            "1.0\n",
            "0.6666666666666666\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.5\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.5\n",
            "0.5\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.5\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.6666666666666666\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.3333333333333333\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "0.5\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.6666666666666666\n",
            "0.5\n",
            "0.0\n",
            "1.0\n",
            "ACC: 0.6797752808988765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wfc7YwwEZOZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}